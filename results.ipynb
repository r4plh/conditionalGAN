{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional GAN Evaluation\n",
    "\n",
    "In this notebook, we will evaluate our trained conditional GAN. The evaluation will be performed on the 'test' split of the CelebA dataset.\n",
    "\n",
    "We will calculate the LPIPS metrics:\n",
    "\n",
    "**Learned Perceptual Image Patch Similarity (LPIPS)**: Measures the perceptual similarity between a generated image and its corresponding real image. Since our GAN is conditional, we generate an image using the embedding of a real image and compare the output to that same real image. Lower is better.\n",
    "\n",
    "**On the test set of CelebA, our conditional GAN achieved competitive LPIPS scores, demonstrating good perceptual similarity between generated and real images.**\n",
    "\n",
    "The complete details of the evaluation process and results can be seen below in this notebook.\n",
    "\n",
    "Result Summary -:\n",
    "\n",
    "**Number of test batches processed: 200\n",
    "LPIPS Score: 0.3981 (Lower is better)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision torcheval lpips facenet-pytorch datasets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "import lpips\n",
    "\n",
    "# Hugging Face dataset library\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Pre-trained encoder\n",
    "from facenet_pytorch import InceptionResnetV1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from facenet_pytorch import InceptionResnetV1\n",
    "\n",
    "class FaceNetEncoder(nn.Module):\n",
    "    \"\"\"A pure PyTorch encoder using a pre-trained FaceNet model.\"\"\"\n",
    "    def __init__(self, device):\n",
    "        super(FaceNetEncoder, self).__init__()\n",
    "        # InceptionResnetV1 pretrained on 'vggface2' provides 512-dim embeddings\n",
    "        self.model = InceptionResnetV1(pretrained='vggface2').to(device)\n",
    "        self.model.eval()\n",
    "\n",
    "    def forward(self, image_batch):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_batch (torch.Tensor): A batch of images of shape (N, C, H, W) normalized to [-1, 1].\n",
    "        Returns:\n",
    "            torch.Tensor: A tensor of face embeddings of shape (N, 512).\n",
    "        \"\"\"\n",
    "        # The model expects images in the range [-1, 1], which matches our data pipeline\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.model(image_batch)\n",
    "        return embeddings\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, noise_dim=100, embedding_dim=512, channels=3):\n",
    "        super(Generator, self).__init__()\n",
    "        input_dim = noise_dim + embedding_dim\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(input_dim, 1024, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(1024, 512, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(512, 256, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(256, 128, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(128, 64, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(64, channels, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, noise, embedding):\n",
    "        combined_input = torch.cat([noise, embedding], dim=1)\n",
    "        reshaped_input = combined_input.view(-1, combined_input.size(1), 1, 1)\n",
    "        return self.main(reshaped_input)\n",
    "\n",
    "# Discriminator is not needed for evaluation, but included for completeness.\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, embedding_dim=512, channels=3):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.image_path = nn.Sequential(\n",
    "            nn.Conv2d(channels, 64, 4, 2, 1, bias=False), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(64, 128, 4, 2, 1, bias=False), nn.BatchNorm2d(128), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(128, 256, 4, 2, 1, bias=False), nn.BatchNorm2d(256), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(256, 512, 4, 2, 1, bias=False), nn.BatchNorm2d(512), nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "        self.combined_path = nn.Sequential(\n",
    "            nn.Conv2d(512 + embedding_dim, 1024, 4, 2, 1, bias=False), nn.BatchNorm2d(1024), nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(1024, 1, 4, 1, 0, bias=False), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, image, embedding):\n",
    "        image_features = self.image_path(image)\n",
    "        embedding_reshaped = embedding.view(-1, embedding.size(1), 1, 1)\n",
    "        embedding_expanded = embedding_reshaped.expand(-1, -1, image_features.size(2), image_features.size(3))\n",
    "        combined = torch.cat([image_features, embedding_expanded], dim=1)\n",
    "        output = self.combined_path(combined)\n",
    "        return output.view(-1, 1).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# --- Configuration ---\n",
    "CHECKPOINT_PATH = \"checkpoints/generator_epoch_2.pth\"\n",
    "NOISE_DIM = 100\n",
    "EMBEDDING_DIM = 512\n",
    "IMAGE_SIZE = 128\n",
    "BATCH_SIZE = 32 \n",
    "LIMIT_batches = 200\n",
    "\n",
    "# --- Device Setup ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if not os.path.exists(\"checkpoints\"):\n",
    "    os.makedirs(\"checkpoints\")\n",
    "    print(\"Created 'checkpoints' directory. Make sure to place your model file inside.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generator loaded successfully from checkpoints/generator_epoch_2.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encoder = FaceNetEncoder(device=device)\n",
    "generator = Generator(noise_dim=NOISE_DIM, embedding_dim=EMBEDDING_DIM).to(device)\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_PATH):\n",
    "    print(f\"ERROR: Checkpoint file not found at '{CHECKPOINT_PATH}'\")\n",
    "    print(\"Please make sure the file exists.\")\n",
    "else:\n",
    "    generator.load_state_dict(torch.load(CHECKPOINT_PATH, map_location=device))\n",
    "    generator.eval() # Set to evaluation mode\n",
    "    print(f\"Generator loaded successfully from {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Preparing CelebA test dataloader (streaming) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b042a031b144e9b8d9428b5fc2b14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30301c1829ba4906aadc260c525956bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DataLoader created successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from datasets import load_dataset\n",
    "\n",
    "def get_celeba_test_dataloader(batch_size, image_size):\n",
    "    \"\"\"Creates a DataLoader for the CelebA TEST split.\"\"\"\n",
    "    print(\"--- Preparing CelebA test dataloader (streaming) ---\")\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.CenterCrop(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), # To range [-1, 1]\n",
    "    ])\n",
    "\n",
    "    def transform_example(example):\n",
    "        example['image'] = preprocess(example['image'].convert(\"RGB\"))\n",
    "        return example\n",
    "\n",
    "    # Use the 'test' split of the dataset\n",
    "    dataset = load_dataset(\"flwrlabs/celeba\", split=\"test\", streaming=True)\n",
    "    transformed_dataset = dataset.map(transform_example)\n",
    "    final_dataset = transformed_dataset.with_format(\"torch\")\n",
    "\n",
    "    dataloader = DataLoader(final_dataset, batch_size=batch_size)\n",
    "    print(\"Test DataLoader created successfully!\")\n",
    "    return dataloader\n",
    "\n",
    "test_dataloader = get_celeba_test_dataloader(batch_size=BATCH_SIZE, image_size=IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Calculating LPIPS Score ---\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /Users/0xr4plh/Documents/Machine Learning/Generative Training/invideo/lib/python3.12/site-packages/lpips/weights/v0.1/alex.pth\n",
      "--- Preparing CelebA test dataloader (streaming) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d93baa80dcb4ff98280bf2e26c41935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bfc1636d5dd4ad794ce7cf526cba81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test DataLoader created successfully!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea878aa7e9e541e2ad18bc424b90f0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Calculating LPIPS:   0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> Final LPIPS Score (Average): 0.3981\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Calculating LPIPS Score ---\")\n",
    "\n",
    "lpips_metric = lpips.LPIPS(net='alex').to(device)\n",
    "all_lpips_scores = []\n",
    "test_dataloader_lpips = get_celeba_test_dataloader(batch_size=BATCH_SIZE, image_size=IMAGE_SIZE)\n",
    "\n",
    "pbar = tqdm(test_dataloader_lpips, total=LIMIT_batches, desc=\"Calculating LPIPS\")\n",
    "\n",
    "for i, batch in enumerate(pbar):\n",
    "    if LIMIT_batches is not None and i >= LIMIT_batches:\n",
    "        break\n",
    "\n",
    "    real_images = batch['image'].to(device)\n",
    "    current_batch_size = real_images.size(0)\n",
    "\n",
    "    # Generate fake images using the same process as before\n",
    "    real_embeddings = encoder(real_images)\n",
    "    noise = torch.randn(current_batch_size, NOISE_DIM, device=device)\n",
    "    fake_images = generator(noise, real_embeddings)\n",
    "    distances = lpips_metric(real_images.detach(), fake_images.detach())\n",
    "    all_lpips_scores.extend(distances.squeeze().detach().cpu().numpy())  # Added .detach() here\n",
    "\n",
    "# Calculate the mean LPIPS score\n",
    "mean_lpips_score = np.mean(all_lpips_scores)\n",
    "print(f\"\\n>>> Final LPIPS Score (Average): {mean_lpips_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Evaluation Complete ---\n",
      "Model: checkpoints/generator_epoch_2.pth\n",
      "Number of test batches processed: 200\n",
      "LPIPS Score: 0.3981 (Lower is better)\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Evaluation Complete ---\")\n",
    "print(f\"Model: {CHECKPOINT_PATH}\")\n",
    "print(f\"Number of test batches processed: {LIMIT_batches if LIMIT_batches is not None else 'All'}\")\n",
    "print(f\"LPIPS Score: {mean_lpips_score:.4f} (Lower is better)\")\n",
    "print(\"-\" * 30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "invideo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
